# real_estate_assistant/agents/property_retriever.py - Ò®Ğ» Ñ…Ó©Ğ´Ğ»Ó©Ñ… Ñ…Ó©Ñ€Ó©Ğ½Ğ³Ğ¸Ğ¹Ğ½ Ğ¼ÑĞ´ÑÑĞ»ÑĞ» Ğ°Ğ²Ğ°Ğ³Ñ‡
import logging
import asyncio
from collections import defaultdict
import json
from typing import Dict, Any, List

import httpx
from bs4 import BeautifulSoup
from langchain_core.documents import Document

from utils.unegui_scraper import UneguiScraper
from data_processors.property_aggregator import PropertyAggregator
from config.constants import DISTRICT_URL_PATHS, BASE_LISTING_URL, MAX_PAGES_TO_SCRAPE_PER_DISTRICT

logger = logging.getLogger(__name__)

class PropertyRetriever:
    def __init__(self, llm=None): # LLM Ğ½ÑŒ ÑĞ½Ñ ÑˆĞ¸Ğ½ÑÑ‡Ğ¸Ğ»ÑÑĞ½ ĞºĞ»Ğ°ÑÑĞ´ ÑˆÑƒÑƒĞ´ Ğ°ÑˆĞ¸Ğ³Ğ»Ğ°Ğ³Ğ´Ğ°Ñ…Ğ³Ò¯Ğ¹, Ğ³ÑÑ…Ğ´ÑÑ Ğ½Ğ¸Ğ¹Ñ†Ñ‚ÑĞ¹ Ğ±Ğ°Ğ¹Ğ»Ğ³Ğ°Ñ…Ñ‹Ğ½ Ñ‚ÑƒĞ»Ğ´ Ò¯Ğ»Ğ´ÑÑÑÑĞ½
        self.llm = llm # Ğ¥ÑÑ€ÑĞ² LLM ÑĞ½Ğ´ Ğ°ÑˆĞ¸Ğ³Ğ»Ğ°Ğ³Ğ´Ğ°Ñ…Ğ³Ò¯Ğ¹ Ğ±Ğ¾Ğ» ÑƒÑÑ‚Ğ³Ğ°Ğ¶ Ğ±Ğ¾Ğ»Ğ½Ğ¾
        self.scraper = UneguiScraper()
        self.aggregator = PropertyAggregator()
        self.district_url_paths = DISTRICT_URL_PATHS

    async def retrieve_property_details(self, url: str) -> Dict[str, Any]:
        """
        Unegui.mn-Ğ¸Ğ¹Ğ½ Ò¯Ğ» Ñ…Ó©Ğ´Ğ»Ó©Ñ… Ñ…Ó©Ñ€Ó©Ğ½Ğ³Ğ¸Ğ¹Ğ½ Ğ´ÑĞ»Ğ³ÑÑ€ÑĞ½Ğ³Ò¯Ğ¹ Ğ¼ÑĞ´ÑÑĞ»Ğ»Ğ¸Ğ¹Ğ³ UneguiScraper Ğ°ÑˆĞ¸Ğ³Ğ»Ğ°Ğ½ Ğ°Ğ²Ñ‡ Ğ·Ğ°Ğ´Ğ»Ğ°Ğ½Ğ°.
        """
        return await self.scraper.retrieve_property_details(url)

    async def retrieve_vector_data(self) -> List[Document]:
        """
        Unegui.mn Ğ¶Ğ°Ğ³ÑĞ°Ğ°Ğ»Ñ‚Ñ‹Ğ½ Ñ…ÑƒÑƒĞ´Ğ°ÑĞ½ÑƒÑƒĞ´Ğ°Ğ°Ñ Ğ±Ğ¾Ğ´Ğ¸Ñ‚ Ñ†Ğ°Ğ³Ğ¸Ğ¹Ğ½ Ğ¼ÑĞ´ÑÑĞ»ÑĞ» Ñ†ÑƒĞ³Ğ»ÑƒÑƒĞ»Ğ¶,
        Ğ´Ò¯Ò¯Ñ€ÑĞ³ Ğ±Ğ¾Ğ»Ğ¾Ğ½ Ó©Ñ€Ó©Ó©Ğ½Ğ¸Ğ¹ Ñ‚Ó©Ñ€Ğ»Ó©Ó©Ñ€ Ğ´ÑƒĞ½Ğ´Ğ°Ğ¶ Ò¯Ğ½Ğ¸Ğ¹Ğ³ Ğ½ÑĞ³Ñ‚Ğ³ÑĞ½,
        DistrictAnalyzer-Ğ¸Ğ¹Ğ½ Ğ²ĞµĞºÑ‚Ğ¾Ñ€ ÑĞ°Ğ½Ğ´ Ğ·Ğ¾Ñ€Ğ¸ÑƒĞ»ÑĞ°Ğ½ Ğ‘Ğ°Ñ€Ğ¸Ğ¼Ñ‚ÑƒÑƒĞ´Ñ‹Ğ³ Ğ±ÑƒÑ†Ğ°Ğ°Ğ½Ğ°.
        """
        aggregated_data = defaultdict(lambda: defaultdict(lambda: {'total_price_per_sqm': 0.0, 'count': 0}))

        logger.info("Starting real-time data collection from Unegui.mn...")

        for district_name, path_segment in self.district_url_paths.items():
            logger.info(f"Collecting data for district: {district_name}")

            district_url = BASE_LISTING_URL + path_segment
            max_pages = MAX_PAGES_TO_SCRAPE_PER_DISTRICT

            for page_num in range(1, max_pages + 1):
                page_url = district_url
                if page_num > 1:
                    page_url += f"&page={page_num}"

                logger.info(f"  {page_num} Ğ´Ğ°Ñ…ÑŒ Ñ…ÑƒÑƒĞ´ÑÑ‹Ğ³ {district_name} Ğ´Ò¯Ò¯Ñ€Ğ³Ğ¸Ğ¹Ğ½ Ñ…ÑƒĞ²ÑŒĞ´ ÑĞºÑ€Ğ°Ğ¿Ğ¸Ğ½Ğ³ Ñ…Ğ¸Ğ¹Ğ¶ Ğ±Ğ°Ğ¹Ğ½Ğ°: {page_url}")

                try:
                    response = await self.scraper.async_client.get(page_url)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.text, "html.parser")

                    listings = soup.find_all("div", class_="advert js-item-listing")
                    logger.info(f"  {page_num} Ğ´Ğ°Ñ…ÑŒ Ñ…ÑƒÑƒĞ´ÑĞ°Ğ½Ğ´ {len(listings)} Ğ¼ÑĞ´ÑÑĞ»ÑĞ» Ğ¾Ğ»Ğ´ÑĞ¾Ğ½")

                    for listing in listings:
                        try:
                            prop_data = self.scraper.extract_listing_data(listing)
                            prop_data['scraped_district'] = district_name

                            logger.debug(f"  Raw extracted data: {json.dumps(prop_data, ensure_ascii=False, indent=2)}")

                            if self.aggregator._is_valid_residential_property(prop_data):
                                self.aggregator.aggregate_property_data(prop_data, aggregated_data)
                                logger.info(f"  âœ… Added residential apartment: {prop_data.get('title', 'N/A')[:50]}...")
                            else:
                                property_type = self.aggregator._classify_property_type(prop_data)
                                logger.info(f"  âŒ Excluded {property_type}: {prop_data.get('title', 'N/A')[:50]}...")

                        except Exception as e:
                            logger.error(f"  Error processing listing: {e}")
                            continue

                    await asyncio.sleep(1) # Delay between page requests

                except httpx.RequestError as e:
                    logger.error(f"  Network error fetching {page_url}: {e}")
                    break
                except Exception as e:
                    logger.error(f"  Error processing page {page_url}: {e}")
                    continue

            logger.info(f"Finished collecting data for {district_name}")

        district_documents = self.aggregator.generate_district_documents(aggregated_data)

        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š REAL-TIME DATA COLLECTION SUMMARY")
        logger.info("=" * 80)

        if not district_documents:
            logger.warning("âŒ No valid residential apartment data was collected!")
            logger.info("This could be due to:")
            logger.info("- Network connectivity issues")
            logger.info("- Website structure changes")
            logger.info("- All properties being filtered out")
            return []

        total_properties = sum(
            room_types.get('overall', {}).get('count', 0)
            for room_types in aggregated_data.values()
        )

        logger.info(f"âœ… Successfully collected data from {len(district_documents)} districts")
        logger.info(f"ğŸ“ˆ Total residential apartments analyzed: {total_properties}")

        for district, room_types in aggregated_data.items():
            overall_info = room_types.get('overall', {'total_price_per_sqm': 0, 'count': 0})
            two_room_info = room_types.get('2_rooms', {'total_price_per_sqm': 0, 'count': 0})
            three_room_info = room_types.get('3_rooms', {'total_price_per_sqm': 0, 'count': 0})

            overall_avg = overall_info['total_price_per_sqm'] / overall_info['count'] if overall_info['count'] > 0 else 0

            logger.info(f"\nğŸ¢ {district}:")
            logger.info(f"   ğŸ“Š Total apartments: {overall_info['count']}")
            logger.info(f"   ğŸ’° Average price/mÂ²: {overall_avg:,.0f} â‚®")
            logger.info(f"   ğŸ  2-room apartments: {two_room_info['count']}")
            logger.info(f"   ğŸ  3-room apartments: {three_room_info['count']}")

        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“‹ GENERATED VECTOR STORE DOCUMENTS")
        logger.info("=" * 80)

        for i, doc in enumerate(district_documents, 1):
            logger.info(f"\nğŸ“„ Document {i}:")
            logger.info("-" * 40)
            logger.info(doc.page_content)
            logger.info("-" * 40)

        logger.info(f"\nâœ… Vector data collection completed successfully!")
        logger.info(f"ğŸ“¦ Generated {len(district_documents)} documents for DistrictAnalyzer")
        logger.info("=" * 80)

        return district_documents

    async def close(self):
        """Ğ¡ĞºÑ€Ğ°Ğ¿ĞµÑ€ Ğ´Ğ°Ñ…ÑŒ async ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¸Ğ¹Ğ³ Ñ…Ğ°Ğ°Ğ½Ğ°."""
        await self.scraper.close()