# real_estate_assistant/agents/district_analyzer.py
import logging
import os
from datetime import datetime, timedelta
import pickle
from pathlib import Path
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_together import ChatTogether
from langchain_community.vectorstores import FAISS
from langchain_together.embeddings import TogetherEmbeddings
from langchain_core.documents import Document

logger = logging.getLogger(__name__)


class DistrictAnalyzer:
    def __init__(self, llm: ChatTogether, property_retriever=None):
        self.llm = llm
        self.property_retriever = property_retriever
        self.embeddings_model = TogetherEmbeddings(
            together_api_key=os.getenv("TOGETHER_API_KEY"),
            model="togethercomputer/m2-bert-80M-8k-retrieval"
        )
        self.vectorstore = None
        self.cache_dir = Path("cache")
        self.cache_dir.mkdir(exist_ok=True)
        self.documents_cache_file = self.cache_dir / "district_documents.pkl"
        self.last_update_file = self.cache_dir / "last_update.txt"

        self._initialize_vectorstore()

    def _initialize_vectorstore(self):
        """Initialize vectorstore with cached data if available and fresh, otherwise use static data"""
        # Check if we have valid cached data
        if self._should_use_cached_data():
            logger.info("üì¶ Loading cached vectorstore data...")
            if self._load_cached_vectorstore():
                logger.info("‚úÖ Successfully loaded cached vectorstore data")
                return
            else:
                logger.warning("‚ö†Ô∏è  Failed to load cached data, falling back to static data")

        # Use static fallback data
        logger.info("üìö Initializing with static fallback data...")
        static_district_data = [
            Document(page_content="""
            –î“Ø“Ø—Ä—ç–≥: –•–∞–Ω-–£—É–ª
            –ù–∏–π—Ç –±–∞–π—Ä–Ω—ã 1–º2 –¥—É–Ω–¥–∞–∂ “Ø–Ω—ç: 4 000 323 —Ç”©–≥—Ä”©–≥
            2 ”©—Ä”©”© –±–∞–π—Ä–Ω—ã 1–º2 –¥—É–Ω–¥–∞–∂ “Ø–Ω—ç: 4 100 323 —Ç”©–≥—Ä”©–≥
            3 ”©—Ä”©”© –±–∞–π—Ä–Ω—ã 1–º2 –¥—É–Ω–¥–∞–∂ “Ø–Ω—ç: 3 900 323 —Ç”©–≥—Ä”©–≥
            –•–∞–Ω-–£—É–ª –¥“Ø“Ø—Ä—ç–≥ –Ω—å –£–ª–∞–∞–Ω–±–∞–∞—Ç–∞—Ä —Ö–æ—Ç—ã–Ω –±–∞—Ä—É—É–Ω —É—Ä–¥ –±–∞–π—Ä–ª–∞–¥–∞–≥. –≠–Ω—ç –¥“Ø“Ø—Ä—ç–≥ –Ω—å –æ—Ä–æ–Ω —Å—É—É—Ü–Ω—ã “Ø–Ω—ç —Ö–∞—Ä—å—Ü–∞–Ω–≥—É–π ”©–Ω–¥”©—Ä –±–∞–π–¥–∞–≥.
            """),
            Document(page_content="""
            –î“Ø“Ø—Ä—ç–≥: –ë–∞—è–Ω–≥–æ–ª
            –ù–∏–π—Ç –±–∞–π—Ä–Ω—ã 1–º2 –¥—É–Ω–¥–∞–∂ “Ø–Ω—ç: 3 510 645 —Ç”©–≥—Ä”©–≥
            2 ”©—Ä”©”© –±–∞–π—Ä–Ω—ã 1–º2 –¥—É–Ω–¥–∞–∂ “Ø–Ω—ç: 3 610 645 —Ç”©–≥—Ä”©–≥
            3 ”©—Ä”©”© –±–∞–π—Ä–Ω—ã 1–º2 –¥—É–Ω–¥–∞–∂ “Ø–Ω—ç: 3 410 645 —Ç”©–≥—Ä”©–≥
            –ë–∞—è–Ω–≥–æ–ª –¥“Ø“Ø—Ä—ç–≥ –Ω—å –£–ª–∞–∞–Ω–±–∞–∞—Ç–∞—Ä —Ö–æ—Ç—ã–Ω —Ç”©–≤ —Ö—ç—Å—ç–≥—Ç –æ–π—Ä –±–∞–π—Ä–ª–∞–¥–∞–≥. –≠–Ω—ç –¥“Ø“Ø—Ä—ç–≥ –Ω—å –¥—É–Ω–¥–∞–∂ “Ø–Ω—ç—Ç—ç–π –æ—Ä–æ–Ω —Å—É—É—Ü —ç–ª–±—ç–≥.
            """),
            Document(page_content="""
            –î“Ø“Ø—Ä—ç–≥: –°“Ø—Ö–±–∞–∞—Ç–∞—Ä
            –ù–∏–π—Ç –±–∞–π—Ä–Ω—ã 1–º2 –¥—É–Ω–¥–∞–∂ “Ø–Ω—ç: 4 500 000 —Ç”©–≥—Ä”©–≥
            2 ”©—Ä”©”© –±–∞–π—Ä–Ω—ã 1–º2 –¥—É–Ω–¥–∞–∂ “Ø–Ω—ç: 4 600 000 —Ç”©–≥—Ä”©–≥
            3 ”©—Ä”©”© –±–∞–π—Ä–Ω—ã 1–º2 –¥—É–Ω–¥–∞–∂ “Ø–Ω—ç: 4 400 000 —Ç”©–≥—Ä”©–≥
            –°“Ø—Ö–±–∞–∞—Ç–∞—Ä –¥“Ø“Ø—Ä—ç–≥ –Ω—å —Ö–æ—Ç—ã–Ω —Ö–∞–º–≥–∏–π–Ω “Ø–Ω—ç—Ç—ç–π –±“Ø—Å“Ø“Ø–¥–∏–π–Ω –Ω—ç–≥ –±”©–≥”©”©–¥ —Ç”©–≤–¥”©”© –æ–π—Ä—Ö–æ–Ω.
            """),
            Document(page_content="""
            –î“Ø“Ø—Ä—ç–≥: –ß–∏–Ω–≥—ç–ª—Ç—ç–π
            –ù–∏–π—Ç –±–∞–π—Ä–Ω—ã 1–º2 –¥—É–Ω–¥–∞–∂ “Ø–Ω—ç: 3 800 000 —Ç”©–≥—Ä”©–≥
            2 ”©—Ä”©”© –±–∞–π—Ä–Ω—ã 1–º2 –¥—É–Ω–¥–∞–∂ “Ø–Ω—ç: 3 900 000 —Ç”©–≥—Ä”©–≥
            3 ”©—Ä”©”© –±–∞–π—Ä–Ω—ã 1m2 –¥—É–Ω–¥–∞–∂ “Ø–Ω—ç: 3 700 000 —Ç”©–≥—Ä”©–≥
            –ß–∏–Ω–≥—ç–ª—Ç—ç–π –¥“Ø“Ø—Ä—ç–≥ –Ω—å —Ö–æ—Ç—ã–Ω —Ç”©–≤ —Ö—ç—Å—ç–≥—Ç –æ—Ä—à–¥–æ–≥.
            """),
        ]

        self.vectorstore = FAISS.from_documents(static_district_data, self.embeddings_model)
        logger.info("FAISS vectorstore initialized with static data.")

    def _should_use_cached_data(self) -> bool:
        """Check if cached data exists and is less than 7 days old"""
        if not self.documents_cache_file.exists() or not self.last_update_file.exists():
            logger.info("üìä No cached data found")
            return False

        try:
            with open(self.last_update_file, 'r') as f:
                last_update_str = f.read().strip()

            last_update = datetime.fromisoformat(last_update_str)
            age = datetime.now() - last_update

            if age > timedelta(days=7):
                logger.info(f"üìÖ Cached data is {age.days} days old, needs refresh")
                return False
            else:
                logger.info(f"üìÖ Cached data is {age.days} days old, still fresh")
                return True

        except Exception as e:
            logger.error(f"‚ùå Error checking cache age: {e}")
            return False

    def _load_cached_vectorstore(self) -> bool:
        """Load vectorstore from cached documents"""
        try:
            with open(self.documents_cache_file, 'rb') as f:
                cached_documents = pickle.load(f)

            # Recreate vectorstore from cached documents
            self.vectorstore = FAISS.from_documents(cached_documents, self.embeddings_model)
            logger.info(f"üì¶ Loaded {len(cached_documents)} documents from cache")
            return True
        except Exception as e:
            logger.error(f"‚ùå Error loading cached documents: {e}")
            return False

    def _save_vectorstore_cache(self, documents):
        """Save documents to cache file instead of vectorstore"""
        try:
            with open(self.documents_cache_file, 'wb') as f:
                pickle.dump(documents, f)

            with open(self.last_update_file, 'w') as f:
                f.write(datetime.now().isoformat())

            logger.info("üíæ Documents cached successfully")
            return True
        except Exception as e:
            logger.error(f"‚ùå Error saving documents cache: {e}")
            return False

    async def update_with_realtime_data(self, force_update: bool = False):
        """Update vectorstore with real-time scraped data only if needed"""
        if not self.property_retriever:
            logger.warning("No PropertyRetriever provided. Cannot update with real-time data.")
            return False

        # Check if update is needed
        if not force_update and self._should_use_cached_data():
            logger.info("üìÖ Cached data is still fresh, skipping update")
            return True

        try:
            logger.info("üîÑ Updating vectorstore with real-time data...")
            real_time_documents = await self.property_retriever.retrieve_vector_data()

            if real_time_documents:
                # Create new vectorstore with real-time data
                new_vectorstore = FAISS.from_documents(real_time_documents, self.embeddings_model)

                # Save documents to cache (not the vectorstore object)
                if self._save_vectorstore_cache(real_time_documents):
                    self.vectorstore = new_vectorstore

                logger.info(f"‚úÖ Vectorstore updated with {len(real_time_documents)} real-time documents.")

                # Log what districts we now have data for
                districts = []
                for doc in real_time_documents:
                    lines = doc.page_content.split('\n')
                    district_line = lines[0] if lines else ""
                    if '–î“Ø“Ø—Ä—ç–≥:' in district_line:
                        district_name = district_line.replace('–î“Ø“Ø—Ä—ç–≥:', '').strip()
                        districts.append(district_name)

                logger.info(f"üìç Real-time data available for districts: {', '.join(districts)}")
                return True
            else:
                logger.warning("‚ö†Ô∏è  No real-time data retrieved. Keeping existing vectorstore.")
                return False

        except Exception as e:
            logger.error(f"‚ùå Error updating vectorstore with real-time data: {e}")
            logger.info("Continuing with existing vectorstore data.")
            return False

    async def ensure_fresh_data(self):
        """Ensure we have fresh data, but don't update on every request"""
        if not self._should_use_cached_data():
            logger.info("üîÑ Data is stale or missing, updating...")
            await self.update_with_realtime_data(force_update=True)
        else:
            logger.info("üìä Using cached data (fresh)")

    async def analyze_district(self, location: str) -> str:
        """
        Analyzes district information based on the provided location,
        leveraging cached or real-time data as appropriate.
        """
        logger.info(f"üîç Analyzing district for location: '{location}'")

        # Only update data if it's stale (not on every request)
        await self.ensure_fresh_data()

        # Check what districts are available in vectorstore
        available_docs = list(self.vectorstore.docstore._dict.values())
        available_districts = []
        for doc in available_docs:
            lines = doc.page_content.split('\n')
            district_line = lines[0] if lines else ""
            if '–î“Ø“Ø—Ä—ç–≥:' in district_line:
                district_name = district_line.replace('–î“Ø“Ø—Ä—ç–≥:', '').strip()
                available_districts.append(district_name)

        logger.info(f"üìä Available districts in vectorstore: {', '.join(available_districts)}")

        # Retrieve relevant district info from vectorstore with scores
        retrieved_results = self.vectorstore.similarity_search_with_score(location, k=len(available_docs))

        logger.debug(f"Retrieved {len(retrieved_results)} documents for '{location}'.")

        # Sort by score (lower score is more similar for FAISS cosine distance)
        retrieved_results.sort(key=lambda x: x[1])

        # Find the best match
        best_match_doc = None
        for doc, score in retrieved_results:
            logger.debug(f"  Doc Score: {score:.4f}, Content (first 50 chars): {doc.page_content.strip()[:50]}...")
            if location.lower() in doc.page_content.lower():
                best_match_doc = doc
                logger.info(f"‚úÖ Found direct match for '{location}' with score {score:.4f}.")
                break

        if best_match_doc:
            retrieved_content = best_match_doc.page_content

            # Check if this is real-time data (has timestamp) or static data
            if '–î–∞—Ç–∞ —Ü—É–≥–ª—É—É–ª—Å–∞–Ω –æ–≥–Ω–æ–æ:' in retrieved_content:
                logger.info("üìä Using REAL-TIME scraped data for analysis")
            else:
                logger.info("üìö Using STATIC fallback data for analysis")

            logger.info(f"Retrieved district info: {retrieved_content.splitlines()[0]}...")
            logger.debug(f"Full retrieved_content being passed to LLM: \n{retrieved_content}")
        else:
            # Fallback if no exact district name is found
            retrieved_content = "–¢—É—Å –±–∞–π—Ä—à–∏–ª–¥ —Ö–∞–º–∞–∞—Ä–∞—Ö –¥“Ø“Ø—Ä–≥–∏–π–Ω –º—ç–¥—ç—ç–ª—ç–ª –æ–ª–¥—Å–æ–Ω–≥“Ø–π."
            logger.warning(
                f"‚ùå No exact district information found in vectorstore for: '{location}'. Falling back to generic message.")

        prompt_template = """
        ”®–≥”©–≥–¥—Å”©–Ω –±–∞–π—Ä—à–∏–ª –±–æ–ª–æ–Ω “Ø–Ω–∏–π–Ω –º—ç–¥—ç—ç–ª–ª–∏–π–≥ –∞—à–∏–≥–ª–∞–Ω –¥“Ø“Ø—Ä–≥–∏–π–Ω –¥—É–Ω–¥–∞–∂ “Ø–Ω–∏–π–≥ —Ö–∞—Ä—É—É–ª–∂, –±—É—Å–∞–¥ –¥“Ø“Ø—Ä–≥“Ø“Ø–¥—Ç—ç–π —Ö–∞—Ä—å—Ü—É—É–ª—Å–∞–Ω –º—ç–¥—ç—ç–ª—ç–ª ”©–≥–Ω”© “Ø“Ø.
        –•—ç—Ä—ç–≤ ”©–≥”©–≥–¥—Å”©–Ω "“Æ–Ω–∏–π–Ω –º—ç–¥—ç—ç–ª—ç–ª" —Ö—ç—Å—ç–≥—Ç —Ç—É—Ö–∞–π–Ω "–ë–∞–π—Ä—à–∏–ª"-–∏–π–Ω “Ø–Ω–∏–π–Ω –¥—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π –º—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π –±–æ–ª "–º—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π" –≥—ç–∂ –∑–∞–∞–Ω–∞ —É—É.

        –ë–∞–π—Ä—à–∏–ª: {location}
        “Æ–Ω–∏–π–Ω –º—ç–¥—ç—ç–ª—ç–ª:
        <context>
        {price_context}
        </context>

        –¢–∞–Ω—ã —Ö–∞—Ä–∏—É–ª—Ç –¥–∞—Ä–∞–∞—Ö —Ñ–æ—Ä–º–∞—Ç—ã–≥ —Ö–∞—Ç—É—É –º”©—Ä–¥”©–Ω”© (–Ω—ç–º—ç–ª—Ç —Ç–µ–∫—Å—Ç–≥“Ø–π–≥—ç—ç—Ä, –∑”©–≤—Ö”©–Ω –ú–æ–Ω–≥–æ–ª —Ö—ç–ª—ç—ç—Ä):
           - –î“Ø“Ø—Ä—ç–≥: [–î“Ø“Ø—Ä–≥–∏–π–Ω –Ω—ç—Ä]
               - –ù–∏–π—Ç –±–∞–π—Ä–Ω—ã 1–º2 –¥—É–Ω–¥–∞–∂ “Ø–Ω—ç: [“Æ–Ω—ç —ç—Å–≤—ç–ª "–º—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π"]
               - 2 ”©—Ä”©”© –±–∞–π—Ä–Ω—ã 1–º2 –¥—É–Ω–¥–∞–∂ “Ø–Ω—ç: [“Æ–Ω—ç —ç—Å–≤—ç–ª "–º—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π"]
               - 3 ”©—Ä”©”© –±–∞–π—Ä–Ω—ã 1–º2 –¥—É–Ω–¥–∞–∂ “Ø–Ω—ç: [“Æ–Ω—ç —ç—Å–≤—ç–ª "–º—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π"]
           - –•–∞—Ä—å—Ü—É—É–ª–∞–ª—Ç: [–ë—É—Å–∞–¥ –¥“Ø“Ø—Ä–≥“Ø“Ø–¥–∏–π–Ω –¥—É–Ω–¥–∞–∂ “Ø–Ω—ç—ç—Å –¥—ç—ç—à —ç—Å–≤—ç–ª –¥–æ–æ—à –±–∞–π–≥–∞–∞ —ç—Å—ç—Ö –º—ç–¥—ç—ç–ª—ç–ª, —ç—Å–≤—ç–ª "—Ö–∞—Ä—å—Ü—É—É–ª–∞—Ö –º—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π"]

        –¢–∞–Ω—ã —Ö–∞—Ä–∏—É–ª—Ç:
        """
        ANALYZE_PROMPT = PromptTemplate.from_template(prompt_template)
        analysis_chain = ANALYZE_PROMPT | self.llm | StrOutputParser()

        logger.debug(
            f"Invoking analysis_chain with location='{location}' and price_context (first 100 chars): '{retrieved_content[:100]}...'")

        response = await analysis_chain.ainvoke({
            "location": location,
            "price_context": retrieved_content
        })
        logger.info(f"District analysis LLM raw response (first 100 chars): {response[:100]}...")
        return response

    def get_all_districts_summary(self) -> str:
        """Get a summary of all districts in the vectorstore"""
        if not self.vectorstore:
            return "–î“Ø“Ø—Ä–≥–∏–π–Ω –º—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π."

        all_docs = list(self.vectorstore.docstore._dict.values())
        summary_parts = []

        for doc in all_docs:
            lines = doc.page_content.strip().split('\n')
            if lines:
                district_line = lines[0].strip()
                summary_parts.append(district_line)

        return "\n".join(summary_parts) if summary_parts else "–î“Ø“Ø—Ä–≥–∏–π–Ω –º—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π."

    def get_cache_status(self) -> dict:
        """Get information about cache status"""
        status = {
            "cache_exists": self.documents_cache_file.exists(),
            "last_update_exists": self.last_update_file.exists(),
            "is_fresh": False,
            "age_days": None,
            "last_update": None
        }

        if status["last_update_exists"]:
            try:
                with open(self.last_update_file, 'r') as f:
                    last_update_str = f.read().strip()

                last_update = datetime.fromisoformat(last_update_str)
                age = datetime.now() - last_update

                status["is_fresh"] = age <= timedelta(days=7)
                status["age_days"] = age.days
                status["last_update"] = last_update_str

            except Exception as e:
                logger.error(f"Error reading cache status: {e}")

        return status

    async def force_update(self):
        """Force an immediate update of the vectorstore data"""
        return await self.update_with_realtime_data(force_update=True)